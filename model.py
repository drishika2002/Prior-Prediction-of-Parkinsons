# -*- coding: utf-8 -*-
"""MinorProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FNY7XCJGT95WUvN5mlQ1d9-5asN6mC0p

# **PRIOR PREDICTION OF NEURODEGENERATIVE DISEASE: PARKINSONâ€™S DISEASE**
"""

# !pip install sklearn
import numpy as np
import pandas as pd
import warnings
import imblearn
import seaborn as sb
from sklearn import svm
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import TomekLinks 
from imblearn.under_sampling import RandomUnderSampler
from math import sqrt
from scipy.stats import mode
import pickle

data=pd.read_csv("parkinsons.txt")
X = data.loc[:,~data.columns.isin(['name','status'])]
y = data.loc[:,'status']
data.head(10)

data.describe()

data["status"].value_counts()

# status - Health status of the subject: 1 - Parkinson's, 0 - healthy 
temp = data["status"].value_counts()
print(temp)

status_graph = pd.DataFrame({"status": temp.index, "values": temp.values})
print(sb.barplot(x = "status", y = "values", data = status_graph))



oversample = SMOTE()
X, y = oversample.fit_resample(X, y)
y.value_counts()

features=data.loc[:,data.columns!='status'].values[:,1:]
labels=data.loc[:,'status'].values
print(labels)

data.info(),
data.shape

print(data.isna().sum())

warnings.simplefilter(action='ignore', category=FutureWarning)

def distributionData(col):
  sb.distplot(data[col])
  plt.show()

for i in list(data.columns)[1:]:
  distributionData(i)

for i in data.columns[1:]:
    fig, ax = plt.subplots()
    ax.boxplot(data[i])
    plt.ylabel(i)

sb.distplot( data[data.status == 0]['spread1'], color = 'r')
sb.distplot( data[data.status == 1]['spread1'], color = 'g')

data.groupby('status').mean()

cols = ["MDVP:Jitter(%)","MDVP:RAP","MDVP:PPQ","Jitter:DDP","NHR","HNR","MDVP:Flo(Hz)","MDVP:Fo(Hz)"]
fig, axs = plt.subplots(ncols = 8,figsize=(16,8))
fig.tight_layout()
for i in range(0,len(cols)):
    sb.boxplot(x='status',y=cols[i],data=data, ax = axs[i])

#train test split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
#X_train = pd.DataFrame(data=X_train)
#y_train = pd.DataFrame(data=y_train)
#X_train = np.random.permutation(X_train)
#y_train = np.random.permutation(y_train)

X_train.corr()

#using pearson correlation
plt.figure(figsize=(15,15))
cor=X_train.corr()
sb.heatmap(cor,annot=True,cmap=plt.cm.CMRmap_r)
plt.show()

def correlation(X_T,threshold):
  column_cor=set()
  corr_matrix=X_T.corr()
  for i in range(len(corr_matrix.columns)):
    for j in range(i):
      if abs(corr_matrix.iloc[i,j])>threshold:
        column_name=corr_matrix.columns[i]
        column_cor.add(column_name)
  return column_cor

corr_features=correlation(X_train,0.9)
len(set(corr_features))

corr_features

X_train.drop(corr_features,axis=1)
X_test.drop(corr_features,axis=1)

"""## FEATURE SCALING

"""

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

def eucledian(p1,p2):
    dist = np.sqrt(np.sum((p1-p2)**2))
    return dist

def predict(X_train, y_train , X_test, k):
    op_labels = []
     
    #Loop through the Datapoints to be classified
    for item in X_test: 
         
        #Array to store distances
        point_dist = []
         
        #Loop through each training Data
        for j in range(len(X_train)): 
            distances = eucledian(np.array(X_train[j,:]) , item) 
            #Calculating the distance
            point_dist.append(distances) 
        point_dist = np.array(point_dist) 
         
        #Sorting the array while preserving the index
        #Keeping the first K datapoints
        dist = np.argsort(point_dist)[:k] 
         
        #Labels of the K datapoints from above
        labels = y_train[dist]
         
        #Majority voting
        lab = mode(labels) 
        lab = lab.mode[0]
        op_labels.append(lab)
 
    return op_labels

# y_pred = predict(X_train, y_train, X_test, 5)
# accuracy_score(y_test, y_pred) * 100

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range = (0,1))
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Training by XGBoost"""

from xgboost import XGBClassifier
model = XGBClassifier().fit(X_train, y_train)
predictions = model.predict(X_test)

cm = confusion_matrix(y_test, predictions)
print(cm)
a1=accuracy_score(y_test, predictions) * 100
a1

a=f1_score(y_test, predictions)
a

"""## Training by KNN"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)

"""Accuracy, Confusion Matrix and F1 Score"""

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
a2=accuracy_score(y_test, y_pred)*100
a2

b=f1_score(y_test, y_pred)
b

"""## Training by kernel SVM"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, y_train)

"""Accuracy, Confusion Matrix and F1 Score"""

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
a3=accuracy_score(y_test, y_pred)*100
a3

c=f1_score(y_test, y_pred)
c

"""

## Training by Random Forest"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

"""Accuracy, Confusion Matrix and F1 Score"""

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
a4=accuracy_score(y_test, y_pred) * 100
a4

d=f1_score(y_test, y_pred)
d

"""##Training by Decision Tree """

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

"""Accuracy, Confusion Matrix and F1 Score"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
a5=accuracy_score(y_test, y_pred)*100
a5

e=f1_score(y_test, y_pred)
e

"""##Training by Logistic Regression """

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

"""Accuracy, Confusion Matrix and F1 Score"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
a6=accuracy_score(y_test, y_pred)*100
a6

f=f1_score(y_test, y_pred)
f

"""##Training by Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

"""Accuracy, Confusion Matrix and F1 Score"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
a7=accuracy_score(y_test, y_pred)*100
a7

g=f1_score(y_test, y_pred)
g

"""##Training by SVM"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)

"""Accuracy, Confusion Matrix and F1 Score"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
a8=accuracy_score(y_test, y_pred)*100
a8

h=f1_score(y_test, y_pred)
h

"""##Graph of Accuracy & F1 score of all algo"""

y_axis=np.array([a1,a2,a3,a4,a5,a6,a7,a8])
x_axis=np.array(["Xgboost","KNN","Kernel Svm","Rand Forest","Decison Tree","Logistic","Naive Bayes","SVM"])

from matplotlib import pyplot as plt
plt.bar(x_axis,y_axis,width=.7)
plt.xticks(rotation = 90)
plt.xlabel("Different Classification Algorithms")
plt.ylabel("ACCURACY")
plt.title("ACCURACY of different Classification Algorithms")

y_axis=np.array([a,b,c,d,e,f,g,h])
x_axis=np.array(["Xgboost","KNN","Kernel Svm","Rand Forest","Decison Tree","Logistic","Naive Bayes","SVM"])

from matplotlib import pyplot as plt
plt.bar(x_axis,y_axis,width=.7)
plt.xticks(rotation = 90)
plt.xlabel("Different Classification Algorithms")
plt.ylabel("F1-Score")
plt.title("F1-Score of different Classification Algorithms")



"""## Cross Validation"""

# 10-fold cross validation
from sklearn.model_selection import cross_val_score

cv2 = cross_val_score(KNeighborsClassifier(), X, y, cv=10)
cv2 = np.mean(cv2)
cv2

cv3 = cross_val_score(SVC(), X, y, cv=10)
cv3 = np.mean(cv3)
cv3

cv4 = cross_val_score(RandomForestClassifier(), X, y, cv=10)
cv4 = np.mean(cv4)
cv4

cv1 = cross_val_score(XGBClassifier(), X, y, cv=10)
cv1 = np.mean(cv1)
cv1

cv5 = cross_val_score(DecisionTreeClassifier(), X, y, cv=10)
cv5 = np.mean(cv5)
cv5

cv6 = cross_val_score(LogisticRegression(), X, y, cv=10)
cv6 = np.mean(cv6)
cv6

cv7 = cross_val_score(GaussianNB(), X, y, cv=10)
cv7 = np.mean(cv7)
cv7

print(X.shape, X_train.shape, X_test.shape)
print(X_train)

y_axis=np.array([cv1,cv2,cv3,cv4,cv5,cv6,cv7])
x_axis=np.array(["Xgboost","KNN","Kernel Svm","Rand Forest","Decison Tree","Logistic","Naive Bayes"])

from matplotlib import pyplot as plt
plt.bar(x_axis,y_axis,width=.7)
plt.xticks(rotation = 90)
plt.xlabel("Different Classification Algorithms")
plt.ylabel("Accuracy-Score")
plt.title("Accuracy of different Classification Algorithms")


pickle.dump(LogisticRegression, open('model.pkl', 'wb'))
model1 = pickle.load(open('model.pkl', 'wb'))